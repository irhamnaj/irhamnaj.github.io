---
published: true
title: Customer Segmentation on Online Retail
collection: ml
layout: single
author_profile: false
read_time: true
categories: [project]
header :
    teaser : "https://irhamnaj.github.io/assets/images/customer-segmentation.png"
comments : true

tags:
  - table of contents
toc: true

sidebar:
    nav: sidebar-sample
---

![image](https://irhamnaj.github.io/assets/images/customer-segmentation.png)

A small startup can afford to target users based on some rules like demographics. But, for companies with sizes as same as Tokopedia and Bukalapak with million daily customers and transaction. they have got to graduate to a more sophisticated method to target their marketing.

One such approach is  Cluster Analysis, Use mathematical methods to discover groups of similar customers based on many criteria such as customer daily habit, demographic, economic condition, and other characteristics.

One major purpose of the above approach is to get the right cluster (group) of data that can be used by marketing team to maximize the company budget for marketing campaigns like discounts or product offers. In this Project, I will then use RFM Analysis and the k-means unsupervised Machine Learning algorithm to group these customers into clusters. 

# dataset Overview
The dataset includes one years of customer data (between december 2010 s/d december 2011), including their purchasing habits and demographic.

here, the information of each columns:
- InvoiceNo: Invoice number. A 6-digit integral number uniquely assigned to each transaction. If this code starts with the letter 'c', it indicates a cancellation.
- StockCode: Product (item) code. A 5-digit integral number uniquely assigned to each distinct product.
- Description: Product (item) name.
- Quantity: The quantities of each product (item) per transaction.
- InvoiceDate: Invice date and time.The day and time when a transaction was generated.
- UnitPrice: Unit price. Product price per unit in sterling (Â£).
- CustomerID: Customer number. A 5-digit integral number uniquely assigned to each customer.
- Country: Country name. The name of the country where a customer resides.

# Problem Statement
Online retail A has million customer transactions on their website that cust across demographics and has very different Purchasing habits. Our goal is to tackle this complexity and automatically segment customers into groups that respond best to a particular marketing campaign.

# Metrics
Unlike Supervised Machine learning, Unsupervised machine learning models don't have clearly metrics to find optimal parameters such as the number of clusters in k-means. But we can use Elbow and Silhouette methods to analyze number of clusters and find best number of clusters.

# Data Wrangling
## Import Library

```python
# library for data analysis
import pandas as pd
import numpy as np

# library for visualization
import matplotlib as mpl
import matplotlib.pyplot as plt
import matplotlib.cm as cm
from matplotlib import ticker
import seaborn as sns

# datetime manipulation
import datetime, nltk, warnings

#intertools
import itertools


#library statistics
from scipy import stats
from scipy.stats import norm
from scipy.stats import skew
from scipy.stats import kurtosis
from scipy.spatial.distance import cdist

#library for market basket analysis
from mlxtend.frequent_patterns import apriori as ap
from mlxtend.frequent_patterns import association_rules

# No warnings about setting value on copy of slice
pd.options.mode.chained_assignment = None


from pathlib import Path

#library for machine learing
from sklearn.preprocessing import PowerTransformer, StandardScaler
from sklearn.cluster import KMeans

#metrics
from sklearn.metrics import silhouette_samples, silhouette_score
```
## Read dataset
you can use pd.read_excel to read data with format xlsx, then use shape to know the dimension of the data

```python
#read_dataset
df = pd.read_excel('/content/drive/My Drive/Online Retail.xlsx',
                    parse_dates=['InvoiceDate'])

#dataset overview
print('data has {} observations and {} features'.format(df.shape[0],
                                                        df.shape[1]))
print("data overview")
df.head()
```
data has 541909 observations and 8 features
data overview


|    |   InvoiceNo | StockCode   | Description                         |   Quantity | InvoiceDate         |   UnitPrice |   CustomerID | Country        |
|---:|------------:|:------------|:------------------------------------|-----------:|:--------------------|------------:|-------------:|:---------------|
|  0 |      536365 | 85123A      | WHITE HANGING HEART T-LIGHT HOLDER  |          6 | 2010-12-01 08:26:00 |        2.55 |        17850 | United Kingdom |
|  1 |      536365 | 71053       | WHITE METAL LANTERN                 |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |
|  2 |      536365 | 84406B      | CREAM CUPID HEARTS COAT HANGER      |          8 | 2010-12-01 08:26:00 |        2.75 |        17850 | United Kingdom |
|  3 |      536365 | 84029G      | KNITTED UNION FLAG HOT WATER BOTTLE |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |
|  4 |      536365 | 84029E      | RED WOOLLY HOTTIE WHITE HEART.      |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |

## Missing Value
Some Machine learning algorithms cannot handle missing value, So, to prevent error, I will calculate how much missing value in each column and decided which strategy should I implemented to handle missing value

```python
#sum missing value
missing_values = df.isnull().sum()

#calculate percent of missing value
percent_missing = df.isnull().sum() / len(df) * 100

#create dataframe
missing_table = pd.concat([missing_values, percent_missing],axis=1).reset_index()

#rename columns
missing_table = missing_table.rename(columns={'index':'name column', 0:'total missing', 1:'percent missing'})
missing_table['percent missing'] = round(missing_table['percent missing'],1)

#sort values
missing_table = missing_table.sort_values('percent missing', ascending=False)

missing_table

```

|    | name column   |   total missing |   percent missing |
|---:|:--------------|----------------:|------------------:|
|  6 | CustomerID    |          135080 |              24.9 |
|  2 | Description   |            1454 |               0.3 |
|  0 | InvoiceNo     |               0 |               0   |
|  1 | StockCode     |               0 |               0   |
|  3 | Quantity      |               0 |               0   |
|  4 | InvoiceDate   |               0 |               0   |
|  5 | UnitPrice     |               0 |               0   |
|  7 | Country       |               0 |               0   |


- CustomerID has a 24.9 percent missing value. Normally, we need to communicate this problem with Data Engineer or marketing team to fill missing customerID with actual CustomerID because remove 24,9 percent of data could omit important information that might be useful for cluster analysis. But, Since we don't have any information, missing value below 30 % and CustomerID contain every information about the customer, we will use data that has a customerID

- i will not change Descreption because i don't need that column for customer segmentation

```python
#remove missing value
new_df =df[pd.notnull(df['CustomerID'])]

new_df.shape
```
(406829, 8)

## Feature Engineering
For this part, i have:

- Create columns contains total price (Unit price multiply with quantity)
- remove quantity with negative value
- create columns contain information about customer is canceled their invoice or not
- split data in some columns, such as year, yearmonth, month, days and hours.

```python

#create columns contain total price
new_df['total price'] = new_df['UnitPrice'] * new_df['Quantity']

#remove quaantity with negative value
new_df = new_df[new_df['Quantity'] > 0]

#create columns to find the customer is canceled their buying or not0
new_df['InvoiceNo'] = new_df['InvoiceNo'].astype('str')

def is_cancel(datas):
  if 'C' in datas or 'c' in datas:
    return 1
  else:
    return 0

new_df['is_canceled'] = new_df['InvoiceNo'].apply(is_cancel)

# add the year, month, day , and hours columns for invoice
new_df['year'] = new_df.InvoiceDate.dt.year
new_df['yearmonth'] = new_df['InvoiceDate'].map(lambda x: 100*x.year + x.month)
new_df['month'] = new_df.InvoiceDate.dt.month
new_df['days'] = new_df.InvoiceDate.dt.dayofweek + 1
new_df['hour'] = new_df.InvoiceDate.dt.hour

new_df.head()
```

|    |   InvoiceNo | StockCode   | Description                         |   Quantity | InvoiceDate         |   UnitPrice |   CustomerID | Country        |   total price |   is_canceled |   year |   yearmonth |   month |   days |   hour |
|---:|------------:|:------------|:------------------------------------|-----------:|:--------------------|------------:|-------------:|:---------------|--------------:|--------------:|-------:|------------:|--------:|-------:|-------:|
|  0 |      536365 | 85123A      | WHITE HANGING HEART T-LIGHT HOLDER  |          6 | 2010-12-01 08:26:00 |        2.55 |        17850 | United Kingdom |         15.3  |             0 |   2010 |      201012 |      12 |      3 |      8 |
|  1 |      536365 | 71053       | WHITE METAL LANTERN                 |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |         20.34 |             0 |   2010 |      201012 |      12 |      3 |      8 |
|  2 |      536365 | 84406B      | CREAM CUPID HEARTS COAT HANGER      |          8 | 2010-12-01 08:26:00 |        2.75 |        17850 | United Kingdom |         22    |             0 |   2010 |      201012 |      12 |      3 |      8 |
|  3 |      536365 | 84029G      | KNITTED UNION FLAG HOT WATER BOTTLE |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |         20.34 |             0 |   2010 |      201012 |      12 |      3 |      8 |
|  4 |      536365 | 84029E      | RED WOOLLY HOTTIE WHITE HEART.      |          6 | 2010-12-01 08:26:00 |        3.39 |        17850 | United Kingdom |         20.34 |             0 |   2010 |      201012 |      12 |      3 |      8 |

# Explanatory Data Analysis
## Customer Analysis
### which customer has the highest number of buying
```python

#choose only customer no canceled
df_no_cancel = new_df[new_df['is_canceled'] == 0]

#choose customerid,country and invoiceNo
n_buying=df_no_cancel[['CustomerID','Country','InvoiceNo']]

#count invoiceNo
n_buying = n_buying.groupby(['CustomerID','Country'])['InvoiceNo'].aggregate('count')
n_buying = n_buying.reset_index().sort_values('InvoiceNo', ascending=False)

n_buying.head(10)
```

|      |   CustomerID | Country        |   InvoiceNo |
|-----:|-------------:|:---------------|------------:|
| 4019 |        17841 | United Kingdom |        7847 |
| 1888 |        14911 | EIRE           |        5677 |
| 1298 |        14096 | United Kingdom |        5111 |
|  334 |        12748 | United Kingdom |        4596 |
| 1670 |        14606 | United Kingdom |        2700 |
| 2185 |        15311 | United Kingdom |        2379 |
| 1698 |        14646 | Netherlands    |        2080 |
|  570 |        13089 | United Kingdom |        1818 |
|  699 |        13263 | United Kingdom |        1677 |
| 1443 |        14298 | United Kingdom |        1637 |

some CustomerID bought a huge number of product from retail, maybe this customerID is loyalist buyer from online retail and they love to buy product from online retail

### Which customer spend the most money ?

```python
#choose no_cancel
most_money = df_no_cancel[['CustomerID','Country','total price']]

#sum total price
most_money = most_money.groupby(['CustomerID','Country'])['total price'].aggregate('sum')
most_money = most_money.reset_index().sort_values('total price', ascending=False)

most_money.head(10)
```

|      |   CustomerID | Country        |   total price |
|-----:|-------------:|:---------------|--------------:|
| 1698 |        14646 | Netherlands    |      280206   |
| 4210 |        18102 | United Kingdom |      259657   |
| 3737 |        17450 | United Kingdom |      194551   |
| 3017 |        16446 | United Kingdom |      168472   |
| 1888 |        14911 | EIRE           |      143825   |
|   57 |        12415 | Australia      |      124915   |
| 1342 |        14156 | EIRE           |      117380   |
| 3780 |        17511 | United Kingdom |       91062.4 |
| 2711 |        16029 | United Kingdom |       81024.8 |
|    0 |        12346 | United Kingdom |       77183.6 |

Customer who spend the highest money from online retail very different with first insight, maybe this person is the richest person between customers.

### how many orders per month?

```python
#count invoice depends on month
data_month = df_no_cancel.groupby('InvoiceNo')['yearmonth'].unique().value_counts().sort_index()

#create new dataframe
data_month = pd.DataFrame(data_month).reset_index()

#rename
data_month = data_month.rename(columns={'index':'yearmonth','yearmonth':'total order'})
data_month['yearmonth'] = data_month['yearmonth'].astype('str')

#plot the data
fig, ax = plt.subplots(figsize=(14,6), facecolor='white')
ax.set_facecolor("ghostwhite")
ax = sns.barplot(x = "yearmonth",
                 y = "total order",
                 data=data_month)
plt.title("total order per month", fontsize=25,
          fontweight='semibold',
          pad=25)
plt.xlabel("Month", fontsize=16)
plt.ylabel("total n order", fontsize=16)
ax.set_xticklabels(('Dec_10','Jan_11','Feb_11','Mar_11','Apr_11','May_11',
                    'Jun_11','July_11','Aug_11','Sep_11','Oct_11','Nov_11',
                    'Dec_11'), rotation='horizontal', fontsize=13)

for bar in ax.patches:
    if bar.get_height() == max(data_month['total order']):
        bar.set_color('red')    
    else:
        bar.set_color('grey')


plt.show()
```
![image](https://irhamnaj.github.io/assets/images/total_order.png)

it turns out that November has the highest number of invoices of all months. Perhaps, this pattern occurred because of the Christmas and Christmas holidays in December

### how many orders per day?

```python
#use data customer who not cancel his invoice
data_day = df_no_cancel.groupby('InvoiceNo')['days'].unique().value_counts().sort_index()

#create new dataframe
data_day = pd.DataFrame(data_day).reset_index()

#rename
data_day = data_day.rename(columns={'index':'day','days':'total order'})
data_day['day'] = data_day['day'].astype('str')

#plot the data
fig, ax = plt.subplots(figsize=(14,6), facecolor='white')
ax.set_facecolor("ghostwhite")


ax = sns.barplot(x = "day",
                 y = "total order",
                 data=data_day)
plt.title("total number order per day", fontsize=25,
          fontweight='semibold',
          pad=25)
plt.xlabel("days", fontsize=16)
plt.ylabel("total n order", fontsize=16)
ax.set_xticklabels((('Monday','Tuesday','Wednesday',
                     'Thursday','Friday','Saturday')), rotation='horizontal', fontsize=13)

for bar in ax.patches:
    if bar.get_height() == max(data_day['total order']):
        bar.set_color('red')    
    else:
        bar.set_color('grey')


plt.show()
```
![image](https://irhamnaj.github.io/assets/images/total_order_per_day.png)

The majority of invoices in the dataset are from Thursday. 

### How many order per hours?

```python
#get data hour
data_hour = df_no_cancel.groupby('InvoiceNo')['hour'].unique().value_counts().iloc[:-1].sort_index()

#create hour
def create_hour_time(hour):
    hour = str(hour)
    if len(hour) == 1:
        new_hour = '0{}:00'.format(hour)
    else:
        new_hour = '{}:00'.format(hour)
        
    return new_hour


#create new dataframe
data_hour = pd.DataFrame(data_hour).reset_index()

#rename
data_hour = data_hour.rename(columns={'index':'hour','hour':'total order'})
data_hour['hour'] = data_hour['hour'].astype('str').apply(lambda x: create_hour_time(x[1:-1]))

#plot the data
fig, ax = plt.subplots(figsize=(14,6), facecolor='white')
ax.set_facecolor("ghostwhite")

ax = sns.barplot(x = "hour",
                 y = "total order",
                 data=data_hour)

for bar in ax.patches:
    if bar.get_height() == max(data_hour['total order']):
        bar.set_color('red')    
    else:
        bar.set_color('grey')



ax.spines['bottom'].set_color('darkgray')

plt.ylabel('total invoice', labelpad=15, fontsize=20)
plt.xlabel('hours', labelpad=10, size=20)
plt.title("total number order per hour",
          fontsize=25,
          fontweight='semibold',
          pad=25)

plt.show()
```
![image](https://irhamnaj.github.io/assets/images/total_order_per_hour.png)

we can see that most of the customers tend to buy something at afternoon between 12:00 to 13:00

### Favorite Products

```python
## groupby description
df_d = new_df[['Description', 'InvoiceNo']]
df_d = df_d.groupby('Description')['InvoiceNo'].aggregate('count')

#create new dataframe
df_d = pd.DataFrame(df_d).reset_index()
df_d = df_d.rename(columns={'InvoiceNo':'total booking'})
df_d = df_d.sort_values('total booking').set_index('Description')

fig, ax = plt.subplots(facecolor='white')
ax.set_facecolor("ghostwhite")

df_d['total booking'].tail(7).plot.barh(figsize=(15,6.4),
                            rot=0,
                            alpha=.6)

ax.tick_params(axis='both', left=False, bottom=False, labelsize='14')

ax.spines['bottom'].set_color('darkgray')
plt.grid(False, axis='x')
plt.grid(axis='y',alpha=.5)
plt.ylabel('', labelpad=15, fontsize=20)
plt.xlabel('total booking', labelpad=10, size=20)
plt.title("Top 7 Most Favorite Product by Customers",
          fontsize=25,
          x=.42,
          fontweight='semibold',
          pad=25)



plt.show()
```
![image](https://irhamnaj.github.io/assets/images/favorite_customers.png)

we can find that white hanging is the most favorute items in shop

# Customer Segmentation

According to [Felicia](https://medium.com/the-anatomy-of-marketing/how-to-know-your-customer-77f5f0349258), Customer segmentation is the process of grouping your customers by common attributes or characteristics, which can be demographic or psychographic. The objective is to have a deeper understanding of each segment so you can market and message effectively.

The most common ways in which businesses segment their customer base are:
- Demographic information, such as gender, age, familial and marital status, income, education, and occupation.
- Geographical information, which differs depending on the scope of the company. For localized businesses, this info might pertain to specific towns or counties. For larger companies, it might mean a customer's city, state, or even country of residence.
- Psychographics, such as social class, lifestyle, and personality traits.
- Behavioral data, such as spending and consumption habits, product/service usage, and desired benefits.

Since dataset only contain information about geographic and transaction history, i will use geographic and behavioral segmentation

## Geographic Segmentation (Country)

```python
## groupby country
df_c = new_df[['Country', 'CustomerID']]
df_c = df_c.groupby('Country')['CustomerID'].aggregate('count')

#create new dataframe
df_c = pd.DataFrame(df_c).reset_index()
df_c = df_c.rename(columns={'CustomerID':'Total customer'})
df_c = df_c.sort_values('Total customer', ascending=False).set_index('Country')

df_c.head()
```

| Country        |   Total customer |
|:---------------|-----------------:|
| United Kingdom |           354345 |
| Germany        |             9042 |
| France         |             8342 |
| EIRE           |             7238 |
| Spain          |             2485 |

look's like most of customers came from UK. I will focus on customer from other countries

```python
#choose customer from outside UK
not_uk = df[df['Country'] != 'United Kingdom']

## groupby country
df_country = not_uk[['Country', 'CustomerID']]
df_country = df_country.groupby('Country')['CustomerID'].aggregate('count')

#create new dataframe
df_country = pd.DataFrame(df_country)
df_country = df_country.rename(columns={'CustomerID':'Total customer'})
df_country = df_country.sort_values('Total customer')

fig, ax = plt.subplots(facecolor='white')
ax.set_facecolor("ghostwhite")

df_country['Total customer'].tail(7).plot.barh(figsize=(15,6.4),
                            rot=0,
                            alpha=.6)

ax.tick_params(axis='both', left=False, bottom=False, labelsize='14')

ax.spines['bottom'].set_color('darkgray')
plt.grid(False, axis='x')
plt.grid(axis='y',alpha=.5)
plt.ylabel('', labelpad=15, fontsize=20)
plt.xlabel('total customer', labelpad=10, size=20)
plt.title("Distribuition of customers  according to Countries",
          fontsize=25,
          x=.42,
          fontweight='semibold',
          pad=25)

plt.show()
```
![image](https://irhamnaj.github.io/assets/images/distribution_customer.png)

I found that germany has the highest total customers, followed by France and EIRE, if we want to expant the retail bussiness, we cant start to country with the highest total customers

## Behavioral Segmentation

since the data only have information about sales record and not contain any insight or information about the customers, i will choose RFM analysis for behavioral customers segmentation

according to [datacamp]('https://www.datacamp.com/community/tutorials/introduction-customer-segmentation-python'), RFM (Recency, Frequency, Monetary) analysis is a behavior-based approach grouping customers into segments. It groups the customers on the basis of their previous purchase transactions. How recently, how often, and how much did a customer buy. RFM filters customers into various groups for the purpose of better service. It helps managers to identify potential customers to do more profitable business.

The RFM model will take the transactions of a customer and calculate three important informational attributes about each customer:

- Recency: The value of how recently a customer purchased at the establishment (in this project i define Recency as Number of days since last purchase)
- Frequency: How frequent the customer’s transactions are at the establishment
- Monetary value: The dollar (or pounds in our case) value of all the transactions that the customer made at the establishment

### Prepare the data

```python
new_df.describe()
```

|       |    Quantity |    UnitPrice |   CustomerID |   total price |   is_canceled |          year |   yearmonth |        month |         days |         hour |
|:------|------------:|-------------:|-------------:|--------------:|--------------:|--------------:|------------:|-------------:|-------------:|-------------:|
| count | 397924      | 397924       |    397924    |   397924      |        397924 | 397924        | 397924      | 397924       | 397924       | 397924       |
| mean  |     13.0218 |      3.11617 |     15294.3  |       22.3947 |             0 |   2010.93     | 201101      |      7.61254 |      3.61455 |     12.7282  |
| std   |    180.42   |     22.0968  |      1713.17 |      309.056  |             0 |      0.247829 |     23.8365 |      3.41653 |      1.92827 |      2.27353 |
| min   |      1      |      0       |     12346    |        0      |             0 |   2010        | 201012      |      1       |      1       |      6       |
| 25%   |      2      |      1.25    |     13969    |        4.68   |             0 |   2011        | 201104      |      5       |      2       |     11       |
| 50%   |      6      |      1.95    |     15159    |       11.8    |             0 |   2011        | 201107      |      8       |      3       |     13       |
| 75%   |     12      |      3.75    |     16795    |       19.8    |             0 |   2011        | 201110      |     11       |      5       |     14       |
| max   |  80995      |   8142.75    |     18287    |   168470      |             0 |   2011        | 201112      |     12       |      7       |     20       |

There was no negative value inside the data, so we can continue to create RFM Table

### RFM Table

```python
# Recency
# to get recency(number of day since last purchase),
# i calculate different between date last_purchase with date_purchase before last_purchase
reference_date = new_df.InvoiceDate.max() + datetime.timedelta(days = 1)
last_shop = (reference_date - new_df.InvoiceDate).dt.days
last_shop = pd.DataFrame(new_df.CustomerID).merge(pd.DataFrame(last_shop),left_index=True, right_index=True)
last_shop = last_shop.rename(columns={'InvoiceDate': 'Recency'})

recency = pd.DataFrame(last_shop.groupby('CustomerID')['Recency'].min()).reset_index()

#Frequency
n_shopping = df_no_cancel.groupby(['CustomerID'])['InvoiceNo'].aggregate('count').reset_index()
frequency = n_shopping.rename(columns={'InvoiceNo': 'Frequency'})

#Monetary
total_shopping = df_no_cancel.groupby(['CustomerID'])['total price'].aggregate('sum').reset_index()
monetary = total_shopping.rename(columns={'total price':'Monetary'})

#RFM table
df_rfm = recency.merge(frequency, on='CustomerID').merge(monetary, on='CustomerID')

df_rfm.head(10)
```

|    |   CustomerID |   Recency |   Frequency |   Monetary |
|---:|-------------:|----------:|------------:|-----------:|
|  0 |        12346 |       326 |           1 |   77183.6  |
|  1 |        12347 |         2 |         182 |    4310    |
|  2 |        12348 |        75 |          31 |    1797.24 |
|  3 |        12349 |        19 |          73 |    1757.55 |
|  4 |        12350 |       310 |          17 |     334.4  |
|  5 |        12352 |        36 |          85 |    2506.04 |
|  6 |        12353 |       204 |           4 |      89    |
|  7 |        12354 |       232 |          58 |    1079.4  |
|  8 |        12355 |       214 |          13 |     459.4  |
|  9 |        12356 |        23 |          59 |    2811.43 |


## Clustering for behavioral segmentation

Normally we can perform clustering analysis on RFM tables without using kmeans because in general, RFM tables only consist of 3 tables. For example, sorting data from smallest to largest and then clustering data based on its quartile. But sometimes, using a quartile alone is not enough to get hidden information and requires more or even fewer clusters depending on the condition of the data and the information you want to retrieve from the data. To solve this problem, I will use k-means to get the optimal n_cluster and then use the n_cluster to segment each data in each column. Finally, the cluster results for each column are combined into one to get better information or to get new perceptions about customer segmentation

### Data Preprocessing

### Normality check

when we use kmeans as the main models to find the best clusters, we should consider that k-means using mean to calculate distance and find best central point of cluster. Which means that the k-means is strongly influenced by the variance between the data, too large a variance will result in the center point generated by the k-means algorithm will also be different and tend to produce the wrong position or create bias (central point generate by using the mean). So it is necessary to assume that each data must have the same variance (variance=1) that can be generated through normalizing the data, or the data must be processed so that it follows the assumption of a normal distribution.

```python
def normality_check(columns,colour='dodgeblue'):

  #plot distrbution
  plt.figure(figsize=(15,8))
  plt.subplot(221)
  sns.distplot(df_rfm[columns],fit=norm,bins=20)
  plt.title('distribution')


  #QQ plot
  plt.subplot(222)
  stats.probplot(df_rfm[columns], plot=plt)

  plt.suptitle(columns, size=18, x=0.51, y=0.96)

  plt.show() 

col_rfm = ['Recency', 'Frequency', 'Monetary']

#looping
for x in col_rfm:
  normality_check(x)

```

![image](https://irhamnaj.github.io/assets/images/check_recency.png)

![image](https://irhamnaj.github.io/assets/images/check_frequency.png)

![image](https://irhamnaj.github.io/assets/images/check_monetary.png)

Overall, there was two insight from visualizatin above:
- frow distribution plot, it shows that all of the columns showing not-normal distribution and tend to have the condition of right skewness and high kurtosis (heavy-tailed distribution). Particularly frequency and monetary column, both have extreme right skewness and kurtosis.
- In Probability plot or Q-Q plot, data from three columns doesn't follow a straight red line, which indicates that all columns don't follow normal distribution. to know more about Q-Q plot, you can click this [link](https://data.library.virginia.edu/understanding-q-q-plots/#:~:text=A%20Q%2DQ%20plot%20is%20a,a%20line%20that's%20roughly%20straight.)


### deal with not-normal distribution

The visualization above show that the data is not follow normal asumtion. To solve that problem, I will transform all three columns. After that, rescale using a scaler to make every data in all columns has the same range.

for data transformation, I will use the Sklearn power transformer module. I use this module because this module can change the distribution of data to follow normal distribution assumptions using the box-cox transformation (if the data contains only positive values) or yeoh-johnson transformation (if the data contains positive and negative values). for more information you can see this [link](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PowerTransformer.html)

```python

#copy the data
norm_df_rfm = df_rfm.copy()

#drop customer_ID
norm_df_rfm = norm_df_rfm.drop('CustomerID',axis=1)

#transform the data
power = PowerTransformer().fit(norm_df_rfm)
rfm_scaled =power.transform(norm_df_rfm)

#Standard Scaler to Scale the data
scaler = StandardScaler().fit(rfm_scaled)
rfm_scaled = scaler.transform(rfm_scaled)

#create new dataframe
df_rfm_scaled = pd.DataFrame(rfm_scaled, columns={'Recency', 'Frequency', 'Monetary'})

#check normality data
def normality_check(columns,colour='dodgeblue'):

  #plot distrbution
  plt.figure(figsize=(15,8))
  plt.subplot(221)
  sns.distplot(df_rfm_scaled[columns],fit=norm,bins=20)
  plt.title('distribution')


  #QQ plot
  plt.subplot(222)
  stats.probplot(df_rfm_scaled[columns], plot=plt)

  plt.suptitle(columns, size=18, x=0.51, y=0.96)

  plt.show() 

col_rfm = ['Recency', 'Frequency', 'Monetary']

#looping
for x in col_rfm:
  normality_check(x)
```

![image](https://irhamnaj.github.io/assets/images/recency_scaled.png)

![image](https://irhamnaj.github.io/assets/images/frequency_scaled.png)

![image](https://irhamnaj.github.io/assets/images/monetary_scaled.png)

the picture above shows that all three columns are normally distributed

### K-means Clustering

### Elbow Method

for the first, I will use the elbow method in distortion and inertia to find the optimal number of clusters. The idea behind the elbow method is to identify the value of k where the distortion or inertia begins to increase most rapidly. If k increases, the distortion or inertia will decrease, because the samples will be closer to the centroids they are assigned to.

- Distortion: It is calculated as the average of the squared distances from the cluster centers of the respective clusters. Typically, the Euclidean distance metric is used.
- Inertia: It is the sum of squared distances of samples to their closest cluster center.

```python
inertia = []
map_inertia ={}
cl = 10

#loop i , i represent number_cluster range from 1 to 10
for i in range(1,cl+1):

  #initiate model
  models = KMeans(n_clusters=i,init='k-means++',max_iter=300, n_init=10, random_state=0)

  #fit model to the data
  models.fit(rfm_scaled)
  
  #append inertia value to list
  inertia.append(models.inertia_)

  #mapping the inertia
  map_inertia[i] = models.inertia_ 

#visualize the inertia
for key,value in map_inertia.items():
  print('cluster {} has inertia value {}'.format(key,value))

print('\n')
plt.figure(figsize=(15,8))
plt.plot(range(1,11), inertia, 'bx-') 
plt.xlabel('Values of K') 
plt.ylabel('Inertia') 
plt.title('The Elbow Method using Inertia') 
plt.show()
```
- cluster 1 has inertia value 13017.000000000002
- cluster 2 has inertia value 6754.9749110835955
- cluster 3 has inertia value 5236.309434668392
- cluster 4 has inertia value 4287.7345066338685
- cluster 5 has inertia value 3689.166826374757
- cluster 6 has inertia value 3224.9872984699014
- cluster 7 has inertia value 2939.761075021346
- cluster 8 has inertia value 2721.8103287150543
- cluster 9 has inertia value 2546.5322040256865
- cluster 10 has inertia value 2388.889421519455

![image](https://irhamnaj.github.io/assets/images/elbow.png)

to determine the optimal number of clusters, we have to select the value of k or cluster at the “elbow” ie the point after which the inertia start decreasing in linear visualization above (or we can try to find the value of k which has the highest difference inertia with value k-1 ). For the given data, we can conclude that the candidate of the optimal number of cluster data are 3,4 or 5.

### Silhouette analysis

after got the list of best cluster, i will use shihoutte analysis to find cluster with the best shihoutte value

Silhouette analysis can be used to study the separation distance between the resulting clusters, as a strategy to quantifying the quality of clustering via graphical tool to plot a measure of how tightly grouped the samples in the clusters are. The silhouette plot displays a measure of how close each point in one cluster is to points in the neighboring clusters and thus provides a way to assess parameters like number of clusters visually. Also, we can find optimal n_cluster with calculate average silhoutte_value for every n_clusters and choose n_clusters with the highest average silhoutte value. to know more about Silhoutte analysis, you can use this [link](https://medium.com/@cmukesh8688/silhouette-analysis-in-k-means-clustering-cefa9a7ad111)

```python
range_n_clusters = [3, 4, 5]

for n_clusters in range_n_clusters:
    # Create a subplot with 1 row and 2 columns
    fig, (ax1, ax2) = plt.subplots(1, 2)
    fig.set_size_inches(18, 7)

    # The 1st subplot is the silhouette plot
    # The silhouette coefficient can range from -1, 1 but in this example all
    # lie within [-0.1, 1]
    ax1.set_xlim([-0.1, 1])
    # The (n_clusters+1)*10 is for inserting blank space between silhouette
    # plots of individual clusters, to demarcate them clearly.
    ax1.set_ylim([0, len(rfm_scaled) + (n_clusters + 1) * 10])

    # Initialize the clusterer with n_clusters value and a random generator
    # seed of 10 for reproducibility.
    clusterer = KMeans(n_clusters=n_clusters,init='k-means++',max_iter=300, n_init=10, random_state=0)
    cluster_labels = clusterer.fit_predict(rfm_scaled)

    # The silhouette_score gives the average value for all the samples.
    # This gives a perspective into the density and separation of the formed
    # clusters
    silhouette_avg = silhouette_score(rfm_scaled, cluster_labels)
    print("For n_clusters =", n_clusters,
          "The average silhouette_score is :", silhouette_avg)

    # Compute the silhouette scores for each sample
    sample_silhouette_values = silhouette_samples(rfm_scaled, cluster_labels)

    y_lower = 10
    for i in range(n_clusters):
        # Aggregate the silhouette scores for samples belonging to
        # cluster i, and sort them
        ith_cluster_silhouette_values = \
            sample_silhouette_values[cluster_labels == i]

        ith_cluster_silhouette_values.sort()

        size_cluster_i = ith_cluster_silhouette_values.shape[0]
        y_upper = y_lower + size_cluster_i

        color = cm.nipy_spectral(float(i) / n_clusters)
        ax1.fill_betweenx(np.arange(y_lower, y_upper),
                          0, ith_cluster_silhouette_values,
                          facecolor=color, edgecolor=color, alpha=0.7)

        # Label the silhouette plots with their cluster numbers at the middle
        ax1.text(-0.05, y_lower + 0.5 * size_cluster_i, str(i))

        # Compute the new y_lower for next plot
        y_lower = y_upper + 10  # 10 for the 0 samples

    ax1.set_title("The silhouette plot for the various clusters.")
    ax1.set_xlabel("The silhouette coefficient values")
    ax1.set_ylabel("Cluster label")

    # The vertical line for average silhouette score of all the values
    ax1.axvline(x=silhouette_avg, color="red", linestyle="--")

    ax1.set_yticks([])  # Clear the yaxis labels / ticks
    ax1.set_xticks([-0.1, 0, 0.2, 0.4, 0.6, 0.8, 1])

    # 2nd Plot showing the actual clusters formed
    colors = cm.nipy_spectral(cluster_labels.astype(float) / n_clusters)
    ax2.scatter(rfm_scaled[:, 0], rfm_scaled[:, 1], marker='.', s=30, lw=0, alpha=0.7,
                c=colors, edgecolor='k')

    # Labeling the clusters
    centers = clusterer.cluster_centers_
    # Draw white circles at cluster centers
    ax2.scatter(centers[:, 0], centers[:, 1], marker='o',
                c="white", alpha=1, s=200, edgecolor='k')

    for i, c in enumerate(centers):
        ax2.scatter(c[0], c[1], marker='$%d$' % i, alpha=1,
                    s=50, edgecolor='k')

    ax2.set_title("The visualization of the clustered data.")
    ax2.set_xlabel("Feature space for the 1st feature")
    ax2.set_ylabel("Feature space for the 2nd feature")

    plt.suptitle(("Silhouette analysis for KMeans clustering on sample data "
                  "with n_clusters = %d" % n_clusters),
                 fontsize=14, fontweight='bold')

    plt.show()
```
For n_clusters = 3 The average silhouette_score is : 0.29952912686846084
![image](https://irhamnaj.github.io/assets/images/n_cluster3.png)

For n_clusters = 4 The average silhouette_score is : 0.3044718924350109
![image](https://irhamnaj.github.io/assets/images/n_cluster4.png)

For n_clusters = 5 The average silhouette_score is : 0.27731263762955316
![image](https://irhamnaj.github.io/assets/images/n_cluster5.png)

In all picture, we can see that how plot and score different according to n_clusters. In the left plot, all n_clusters are valid because they don't have the presence of clusters with below-average silhouette scores (The end of the area does not cross or is positioned to the left of the horizontal red line).  The right plot showing how our clusters distributed, but it's hard to choose the optimal n_cluster because the scatter plot can't showing clearly the gap between cluster.

if we see silhoutte score, all our clusters have low silhoutte_score (below 0.35), but we can clearly see that n_cluster 4 has the highest silhouette even though the difference between n_clusters 4 and 3 are very low. So we will choose n_clusters 4 as the optimal cluster.

# Cluster analysis

Firstly, let's create clustering with n_clusters = 4

```python
n = 4

#create model
model = KMeans(n_clusters=n,init='k-means++',max_iter=300, n_init=10, random_state=0)

#fit the data
model_kmeans = model.fit(rfm_scaled)

#labels model
labels = model_kmeans.labels_

#center of models
centers = model_kmeans.cluster_centers_

#Append labels/clusters to scaled and original dataframe
df_rfm['cluster'] = labels

#create new_dataframe
clusters_seg = pd.DataFrame(rfm_scaled,columns=['recency','frequency','monetary'])

#append label to new_dataframe
clusters_seg['Cluster'] = labels+1
```

|    |   CustomerID |   Recency |   Frequency |   Monetary |   cluster |
|---:|-------------:|----------:|------------:|-----------:|----------:|
|  0 |        12346 |       326 |           1 |   77183.6  |         4 |
|  1 |        12347 |         2 |         182 |    4310    |         3 |
|  2 |        12348 |        75 |          31 |    1797.24 |         4 |
|  3 |        12349 |        19 |          73 |    1757.55 |         3 |
|  4 |        12350 |       310 |          17 |     334.4  |         2 |
|  5 |        12352 |        36 |          85 |    2506.04 |         4 |
|  6 |        12353 |       204 |           4 |      89    |         2 |


##  Analysis Statistics






